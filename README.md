# ğŸ§  Neural Networks from Scratch with Math & NumPy

[![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)](https://www.python.org/)
[![NumPy](https://img.shields.io/badge/NumPy-013243?style=for-the-badge&logo=numpy&logoColor=white)](https://numpy.org/)
[![Jupyter](https://img.shields.io/badge/Jupyter-F37626?style=for-the-badge&logo=jupyter&logoColor=white)](https://jupyter.org/)

> **Understanding Neural Networks at the Mathematical Level**  
> A deep dive into neural network fundamentals, implementing forward and backward propagation from scratch using only NumPy and mathematical equations.

---

## ğŸ“Š Project Overview

This project provides a **comprehensive mathematical and practical understanding** of neural networks by implementing them from scratch. No high-level frameworksâ€”just **pure NumPy** and **mathematical equations**.

### Learning Objectives
- ğŸ¯ Understand the math behind neural networks
- ğŸ“ Implement forward propagation manually
- ğŸ”„ Derive and code backpropagation
- ğŸ§® Master gradient descent optimization

---

## ğŸ› ï¸ Tech Stack

- **Core:** Python 3.8+, NumPy
- **Visualization:** Matplotlib
- **Environment:** Jupyter Notebook

---

## ğŸ“ Mathematical Foundation

### Forward Propagation

#### Hidden Layer
```
Z[1] = W[1] Â· X + b[1]
A[1] = ReLU(Z[1]) = max(0, Z[1])
```

#### Output Layer
```
Z[2] = W[2] Â· A[1] + b[2]
A[2] = Softmax(Z[2])
```

### Backward Propagation (Gradients)

```
dZ[2] = A[2] - Y
dW[2] = (1/m) Â· dZ[2] Â· A[1]T
db[2] = (1/m) Â· Î£ dZ[2]

dZ[1] = (W[2]T Â· dZ[2]) âŠ™ ReLU'(Z[1])
dW[1] = (1/m) Â· dZ[1] Â· X.T
db[1] = (1/m) Â· Î£ dZ[1]
```

### Parameter Updates
```
W := W - Î± Â· dW
b := b - Î± Â· db
```

---

## ğŸš€ Getting Started

```bash
pip install numpy matplotlib jupyter

# Clone repository
git clone https://github.com/TirtheshJani/NN-with-math-and-numpy.git
cd NN-with-math-and-numpy

jupyter notebook "NN from scratch wip.ipynb"
```

---

## ğŸ“Š Variable Reference

| Variable | Shape | Description |
|----------|-------|-------------|
| `X` | (784, m) | Input matrix (flattened 28x28 images) |
| `W[1]` | (10, 784) | Hidden layer weights |
| `b[1]` | (10, 1) | Hidden layer biases |
| `A[1]` | (10, m) | Hidden layer activated output |
| `W[2]` | (10, 10) | Output layer weights |
| `b[2]` | (10, 1) | Output layer biases |
| `A[2]` | (10, m) | Final predictions |
| `Y` | (10, m) | One-hot encoded labels |

---

## ğŸ“ License

MIT License - see [LICENSE](LICENSE) file.

---

<p align="center">
  <i>Master the fundamentals, build the future ğŸ§ ğŸ’»</i>
</p>
